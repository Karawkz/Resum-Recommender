{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T20:26:10.630565Z",
     "start_time": "2019-02-03T20:26:10.234062Z"
    }
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "import time\n",
    "import os\n",
    "import re\n",
    "import itertools\n",
    "import pickle\n",
    "from fuzzywuzzy import fuzz\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T20:26:30.504590Z",
     "start_time": "2019-02-03T20:26:30.501435Z"
    }
   },
   "source": [
    "#### Get resumé page links. Each resumé appears on a different webpage.\n",
    "#### Note: you might want to sign in on an account first - you get stopped from scrolling through pages if you don't sign in after a while."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chromedriver = '/Applications/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "driver.get(\"https://resumes.indeed.com/?rsrdr=1&hl=en&co=US\")\n",
    "\n",
    "roles = ['Data Scientist','Data Analyst','Data Engineer']\n",
    "roles_link = []\n",
    "\n",
    "def scrape_pages(first_page, last_page):\n",
    "    for role in roles:\n",
    "\n",
    "        time.sleep(2);\n",
    "        query = driver.find_element_by_xpath('//*[@id=\"input-q\"]')\n",
    "        query.send_keys(f\"{role}\")\n",
    "        time.sleep(1); \n",
    "\n",
    "        temp = driver.find_element_by_xpath('//*[@id=\"content\"]/div/div[2]/div/div[1]/div[2]/div/form/div[3]/button')\n",
    "        temp.click()\n",
    "        time.sleep(5);\n",
    "\n",
    "        # for each page, there are 50 resumé links and this is displayed in the hyperlink\n",
    "        # identify how many resumés there are in the last page and input it into the function\n",
    "        # e.g. first_page will always be = 50 and last_pages could be ~100000 if there are that many resumés\n",
    "        for num in range(first_page, last_page, 50):\n",
    "            soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "            for person in soup.find_all('a', target='_blank'):\n",
    "                try:\n",
    "                    roles_link.append([person['href'],person.text])\n",
    "                except:\n",
    "                    pass\n",
    "                \n",
    "            try:\n",
    "                driver.get(\"https://resumes.indeed.com/search?l=United%20States&q=Software%20Engineer&searchFields=jt&start={}\".format(str(num)))\n",
    "                time.sleep(5);\n",
    "            except:\n",
    "                return \"Error\"\n",
    "\n",
    "        driver.close()\n",
    "    \n",
    "scrape_pages(50, 100000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing duplicated links and storing in pickle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_link.sort()\n",
    "roles_link = [roles_link for roles_link,_ in itertools.groupby(roles_link)]\n",
    "\n",
    "# with open('se_links.pkl','wb') as file:\n",
    "#     pickle.dump(roles_link,file)\n",
    "    \n",
    "# with open('roles_link.pkl','rb') as file:\n",
    "#     roles_link = pickle.load(file)\n",
    "\n",
    "len(roles_link)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Removing links where titles not relevant to \"Data\" or are \"Data Entry\" roles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "roles_target = [link for link in roles_link if (fuzz.partial_ratio(link[1], 'Data') == 100) \\\n",
    "                and ('Data Entry' not in link[1])]\n",
    "\n",
    "len(roles_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping the resumés from their respective webpages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T20:34:19.760512Z",
     "start_time": "2019-02-03T20:34:17.909875Z"
    }
   },
   "outputs": [],
   "source": [
    "chromedriver = '/Applications/chromedriver'\n",
    "os.environ[\"webdriver.chrome.driver\"] = chromedriver\n",
    "driver = webdriver.Chrome(chromedriver)\n",
    "\n",
    "def scrape_resume(resume):\n",
    "    res = {}\n",
    "    driver.get('https://resumes.indeed.com'+ resume[0])\n",
    "    soup = BeautifulSoup(driver.page_source, 'html.parser')\n",
    "\n",
    "    try:\n",
    "        res['Title'] = [resume[1]]\n",
    "    except:\n",
    "        res['Title'] = ['error']\n",
    "    \n",
    "    try:\n",
    "        res['Companies'] = [company.text for company in soup.find_all('span', class_='icl-u-textBold')]\n",
    "    except:\n",
    "        res['Companies'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Resume Summary'] = [[summary.text for summary in soup.find('div', class_='rezemp-ResumeDisplay-body').find('div')][3]]\n",
    "    except:\n",
    "        res['Resume Summary'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Current Location'] = [[country.text for country in soup.find('div', class_='rezemp-u-h5 icl-u-textColor--secondary').find_all('span')][2]]\n",
    "    except:\n",
    "        res['Current Location'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Start Dates'] = [re.compile(\"[A-Z][a-z]* [0-9]+\").findall(str(start.text))[0] for start in \n",
    "                   soup.find_all('div', class_='rezemp-WorkExperience-subtitle')[-1].find_all('div', class_='icl-u-textColor--tertiary')]\n",
    "    except:\n",
    "        res['Start Dates'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Work Experiences'] = [list(exp.children)[2].text for exp in soup.find_all('div', 'rezemp-WorkExperience')]\n",
    "    except:\n",
    "        res['Work Experiences'] = ['error']\n",
    "\n",
    "    degree = []\n",
    "    university = []\n",
    "    in_ = []\n",
    "    for i in soup.find_all('div',class_='rezemp-ResumeDisplaySection-content'):\n",
    "        try:\n",
    "            for x in i.find_all('div',class_=\"rezemp-ResumeDisplay-university\"):\n",
    "                university.append(x.find('span',class_=\"icl-u-textBold\").text)\n",
    "            res['Universities'] = university\n",
    "            for x in i.find_all('span',class_='rezemp-ResumeDisplay-itemTitle'):\n",
    "                degree.append(x.find('span').text)\n",
    "                in_.append(x.find_all('span')[2].text)\n",
    "            res['Degrees'] = degree\n",
    "            res['in'] = in_\n",
    "        except:\n",
    "            res['Universities'] = ['error']\n",
    "            res['Degrees'] = ['error']\n",
    "            res['in'] = ['error']\n",
    "\n",
    "    skill = []\n",
    "    try:\n",
    "        for i in soup.find_all('div',class_='rezemp-ResumeDisplaySection-content'):\n",
    "            for x in i.find_all('span'):\n",
    "                for y in x.find_all('span'):\n",
    "                    for z in y.find_all('span'):\n",
    "                        skill.append(z.text)\n",
    "        res['Skills'] = skill\n",
    "    except:\n",
    "        res['Skills'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Additionals'] = [[add.text for add in soup.find_all('div',class_='rezemp-ResumeDisplaySection')][-1]]\n",
    "    except:\n",
    "        res['Additionals'] = ['error']\n",
    "\n",
    "    try:\n",
    "        res['Whole Resume'] = [text.text for text in soup.find('div',class_='rezemp-ResumeDisplay')]\n",
    "    except:\n",
    "        res['Whole Resume'] = ['error']\n",
    "\n",
    "    time.sleep(5)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Put each resumé entry into a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T20:35:03.228459Z",
     "start_time": "2019-02-03T20:35:03.221668Z"
    }
   },
   "outputs": [],
   "source": [
    "def dic(lis):\n",
    "    for resume in lis:\n",
    "        try:\n",
    "            list_of_resumes[resume[0]] = scrape_resume(resume)\n",
    "        except:\n",
    "            list_of_resumes[resume[0]] = 'Error'\n",
    "\n",
    "dic(roles_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Making sure that all resumes are including - removing errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-03T20:49:03.019873Z",
     "start_time": "2019-02-03T20:49:03.009770Z"
    }
   },
   "outputs": [],
   "source": [
    "def if_error(lis):\n",
    "    roles_target_e = []\n",
    "    error_resumes = []\n",
    "    for val in lis.values():\n",
    "        if val == 'Error':\n",
    "            roles_target_e.append([key,val])\n",
    "    for resume in roles_target_e:\n",
    "        for other_resume in roles_target:\n",
    "            if resume[0] == other_resume[0]:\n",
    "                error_resumes.append(other_resume)\n",
    "    return error_resumes\n",
    "\n",
    "dic(if_error(list_of_resumes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# no more errors left\n",
    "if_error(list_of_resumes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing resumés into dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('resumes_list.pkl','wb') as file:\n",
    "#     pickle.dump(list_of_resumes,file)\n",
    "\n",
    "with open('resumes_list.pkl','rb') as file:\n",
    "    resumes_list = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting resume link into the dictionary\n",
    "for resume in resumes_list.keys():\n",
    "    resumes_list[resume].update({'Link' : resume})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Title': ['Financial Data Analyst'], 'Companies': ['Yoox Net-A-Porter Group', 'Yoox Net-A-Porter Group', '', 'Critical Planning Assoc. LLC', 'Bloomsburg University of Pennsylvania'], 'Resume Summary': ['Goal-oriented team player with experience in various areas within finance, data analysis, sales and customer service. Proven ability to drive process efficiency and financial integrity as evidenced by years of experience leading teams and reporting data metrics. Dedicated and motivated individual seeking a position in a fast-growth company to promote the corporate strategy by achieving and exceeding business goals and objectives.'], 'Current Location': [''], 'Start Dates': ['May 2012'], 'Work Experiences': ['• Formulate sales reports and other metrics using Microsoft Excel and SPSS/SQL software.• Prepare and analyze sales and productivity reports relative to projections with the use of Pivot Tables and VLOOKUP functions.• Record, monitor and report net spend of various customer account sectors.• Achieved yearly productivity/sales 20% of projected target for 2017.• On pace for 10%+ increase in productivity/sales over 2017, representing a 30% increase over two years.', '• Manage and resolve customer inquiries, perform sales and customer relationship tasks for the high-end consumer.• Conducted reports for implementation of business strategies which lead to increased customer acquisition and retention.• Telephone, internet and scheduled in-person sales utilizing Salesforce, LivePerson, Microsoft Office software.• Provide clear communications and instructions to customers and other team members.', 'NET-A-PORTER• Customer care consultant during summer and winter sale seasons.• Handled customer service inquiries via telephone and internet.', '• Private investment start-up company.• Used various Microsoft Excel and other analytical programs to input and analyze investment and financial data.• Created reports and data tables used for financial analysis reporting related to financial markets.'], 'Universities': ['Bloomsburg University of Pennsylvania'], 'Degrees': [\"Bachelor's\"], 'in': ['Business Administration and Marketing'], 'Skills': ['Business Administration and Marketing', 'CUSTOMER SERVICE', '3 years', 'RETAIL SALES', '3 years', 'SALES', '2 years', 'Financial Analysis', '2 years'], 'Additionals': ['Additional InformationSkills• Financial reporting and analysis• Investment strategies• Strategic planning• Problem solving• Interpersonal skills• Business needs analysis• Sales proficiency• Team leadership• Customer service• Microsoft Office, SQL'], 'Whole Resume': [\"Financial Data AnalystFinancial Data Analyst - YOOX NET-A-PORTER GroupRamsey, NJGoal-oriented team player with experience in various areas within finance, data analysis, sales and customer service. Proven ability to drive process efficiency and financial integrity as evidenced by years of experience leading teams and reporting data metrics. Dedicated and motivated individual seeking a position in a fast-growth company to promote the corporate strategy by achieving and exceeding business goals and objectives.Work ExperienceFinancial Data AnalystYoox Net-A-Porter GroupJanuary 2017 to Present• Formulate sales reports and other metrics using Microsoft Excel and SPSS/SQL software.• Prepare and analyze sales and productivity reports relative to projections with the use of Pivot Tables and VLOOKUP functions.• Record, monitor and report net spend of various customer account sectors.• Achieved yearly productivity/sales 20% of projected target for 2017.• On pace for 10%+ increase in productivity/sales over 2017, representing a 30% increase over two years.Sales AdvisorYoox Net-A-Porter GroupJuly 2016 to December 2016• Manage and resolve customer inquiries, perform sales and customer relationship tasks for the high-end consumer.• Conducted reports for implementation of business strategies which lead to increased customer acquisition and retention.• Telephone, internet and scheduled in-person sales utilizing Salesforce, LivePerson, Microsoft Office software.• Provide clear communications and instructions to customers and other team members.Customer Care ConsultantJune 2013 to June 2016NET-A-PORTER• Customer care consultant during summer and winter sale seasons.• Handled customer service inquiries via telephone and internet.Junior AnalystCritical Planning Assoc. LLCMay 2012 to May 2013• Private investment start-up company.• Used various Microsoft Excel and other analytical programs to input and analyze investment and financial data.• Created reports and data tables used for financial analysis reporting related to financial markets.EducationBachelor's in Business Administration and MarketingBloomsburg University of Pennsylvania Bloomsburg, PAMay 2016SkillsCUSTOMER SERVICE (3 years), RETAIL SALES (3 years), SALES (2 years), Financial Analysis (2 years)Additional InformationSkills• Financial reporting and analysis• Investment strategies• Strategic planning• Problem solving• Interpersonal skills• Business needs analysis• Sales proficiency• Team leadership• Customer service• Microsoft Office, SQL\"], 'Link': '/resume/0002cdce16f56ac0?s=l%3D%26q%3DData%2520Analyst%26searchFields%3Djt%26start%3D36050', '_id': ObjectId('5b944b2c9bd8d21bf38085f0')}\n"
     ]
    }
   ],
   "source": [
    "for resume in resumes_list.keys():\n",
    "    print(resumes_list[resume])\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Storing dictionaries in Mongodb - decided on doing this over SQL because 1. There are different numbers of elements in each category and 2. It's convenient because I scraped dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_mongodb(dictionary):\n",
    "    client = MongoClient('localhost', 27017)\n",
    "    db = client['resumes']\n",
    "    resumes = db['resumes']\n",
    "    for resume in dictionary.keys():\n",
    "        resumes.insert_one(dictionary[resume])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "to_mongodb(resumes_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
